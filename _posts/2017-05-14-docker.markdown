---
layout: post
title:  "Docker for development"
date:   2017-05-14 10:30:39 +0600
categories:
tags: Docker
---

I often hear from different people, that after starting using docker it is hard to refuse it.
I absolutely agree with this fact. Docker for development has decreased the number of configuration
 errors in our project, so more time can be spent on coding instead of searching why some part doesn't
 works. It is very effective especially if a project contains different parts that are being edited by
 different developers, for example frontend and backend. In such case it is undesirable to waist
 frontend developer's time for searching what packages are missing (or other problem)
 for running backend. Docker solves such problem. Moreover, a developer is no longer responsible for
 setting environment for running project, so the enter time to a project has been also decreased.
 This article is a small advice: how to start using docker in a project.

### What is expected

The standard way of entering to a new project is following:

1. Preparing environment according to demands (OS and settings)
2. Cloning project
3. Install necessary packages for development (for example, nodejs, gradle, maven, etc..)
4. Preparing project before first start (copying dirs, symlinks, setting configuration)

These actions may take days of time if a project is rather big. Of course, always goes something wrong,
so other people have to help.

Docker way is rather simpler:

1. Cloning project
2. Pulling or building docker image
3. Run container based on this image

After this step running container is like a virtual box that everything is already prepared for
development. It is needed only to run some script to get project running. Moreover, if someone
accidentally has broken something, a container can be restarted to initial state.

In some cases several containers can be used for development. For example, some parts can be extracted
 to separated container, like database or frontend.

### Simple example

The simple example is this [blog][blog]{:target="_blank"}. This blog is a jekyll project
that can be launched from the root folder with command :

{% highlight bash %}
jekyll serve --host 0.0.0.0
{% endhighlight %}

This command makes project available on localhost:4000. Ctrl+C stops server. To run this command I must
have:
* Jekyll installed
* Jekyll plugin for pagination installed
* Source code

To provide it, I create a [Dockerfile][dockerfile]{:target="_blank"} in the root of my project.

{% highlight bash %}
# As a parent image I'm using already prepared ruby image. It allows me
* to skip ruby installation which is needed for jekyll. There are a lof of prepared images on docker hub.
FROM ruby:2.4.1

# Installing necessary packages and creating dir for sources.
RUN gem install jekyll bundler \
    && gem install jekyll-paginate \
    && mkdir /root/src

# Make port 4000 to be available from outside the container.
EXPOSE 4000
{% endhighlight %}

It is a good style to decrease the number of layers - the number of commands in a Dockerfile. That is
why RUN command performs 3 actions. Attention, this dockerfile has a significant defect: the last versions
of packages will be installed. Better to fix versions to avoid appearing some problems after. However,
now it is ok, but keep it in mind.

Then, using this file everyone can build an image:

{% highlight bash %}
#private/myblog is an image name
docker built -t private/myblog .
{% endhighlight %}

Instead of doing this step an image can be downloaded from docker registry. Of course, in such case
someone should build it and push before.

After this, the container can be created using image:

{% highlight bash %}
# insert real folder path instead if [clone-dir]
docker run -i -t -d --name "myblog" -v [clone-dir]:/root/src -p 4000:4000 private/myblog /bin/bash
{% endhighlight %}

This command creates a container with following options:

* myblog - name of new container
* [clone-dir] will be mounted to /root/src of the container file system. It will be like shared folder
in virtual machine, so files on host and container will be always the same.
* Port 4000 of the container will be mapped to port 4000 of host. So localhost:4000 will redirect
to 4000 port of container.
* private/myblog is an image name.
* /bin/bash is a command that will be performed in container.

The last item is rather important. When this command finishes, the container will stop. /bin/bash
never stops, so it guarantees that my container will work until I stop it. The /bin/bash command does
 nothing, so I have to enter in the container and launch the server by myself:

{% highlight bash %}
# get console of the container
docker exec -it myblog /bin/bash

# following actions are performed inside container
cd ~/src
jekyll serve --host 0.0.0.0
{% endhighlight %}
After this the server is available on localhost:4000 on host.

I like this approach for developers, I believe they should control starting server, restart it if
necessary, change configs and etc. But If I'm going to give this container to someone else,
I will use 'docker run' with command, that launches server:

{% highlight bash %}
# insert real folder path instead if [clone-dir]
docker run -i -t -d --name "myblog" -v [clone-dir]:/root/src -p 4000:4000 private/myblog somescript.bash
#It is assumed, somescript.bash contains commands that launch server
{% endhighlight %}

### Make it easier

I don't like writing run command. It is very long, has a lot of options and I have to remember ports,
folders and names. The [Docker compose][dockercompose]{:target="_blank"} project helps to simplify it.

The file docker-compose.yml (in the root) has following structure:
```
version: '2'
services:
  kameleoon:
    image: private/myblog
    ports:
     - "4000:4000"
    volumes:
     - ./:/root/src
    tty: true
#    command: bash somescript.bash
    command: /bin/bash
```

Then, the command below is equal to 'docker run' command, that was presented above.
{% highlight bash %}
docker-compose up
{% endhighlight %}

It is rather easy for users, isn't it?

### Several containers for development

Imagine, a project contains frontend and backend parts. Frontend developers want server to be working,
but doesn't want to know something about it. Moreover, they even don't want to launch it by themselves.
Backend developers also don't want to deal with frontend. Moreover some people don't want to touch both
frontend and backend. And the one responsible developer needs to control all parts of the application.

The idea of development environment is following: The frontend serves at 4200 port. The backend serves
at 8080 port. A proxy is used to redirect api requests from frontend to backend. (4200->8080). I decided
to place frontend and backend in separate containers:

 ![containers](/images/articles/docker-dev/docker-angular-dev.png)

Every container has a volume that contains source code. Backend container is responsible for jetty web
application that is available by 8080 port from outside (including frontend container). Frontend container
 is responsible for angular2 application. Also it contains proxy to redirect requests. So, using
 following configuration, ports 8080 and 4200 are equal for api requests.

I use following file structure:

```
project
    Dockerfile
    docker-compose.yml
    frontend/
        Dockerfile
        proxy-config.json
        ... some frontend sources
    ... some backend sources
```

Frontend is actually a folder in a project. proxy-config.json is a file that describes proxy. It is
an angular cli project specific file, one may use other configured proxy.

This is corresponding docker-compose configuration:
```
version: '2'
services:
  frontend-service:
    build: frontend
    ports:
     - "4200:4200"
    volumes:
     - ./frontend:/root/src
    tty: true
    command: bash /root/src/frontend-start.bash
#    command: /bin/bash
  backend-service:
    build: .
    ports:
     - "8080:8080"
     - "8000:8000"
# port 8000 is using for remote debug
    volumes:
     - ./:/root/src
    tty: true
#    command: /bin/bash
    command: bash /root/src/backend-start.bash
```

When you are using docker compose, you can use internal docker network for accessing to other containers.
In the proxy configuration below I use the name of backend service that was declared in
 docker-compose.yml file.

{% highlight javascript %}
  "/api": {
    "target": "http://backend-service:8080",
    "secure": false
  }
}
{% endhighlight %}

### Conclusion

If you are not using docker for development, these arguments should push you start using it:

* Decreasing time for preparing environment for new developers
* Less probability of some environment problems
* Project behaviour is the same on all machines
* Separate of responsibility for different project parts
* Study of very important technology
* Probability to work on several projects using one computer

About drawbacks, I've found only one: remote debugging. It rather comfortable for java projects
using Idea, but probably it may lead to problems with other stack.

For me, docker was a step to creating clusters with micrsoservices. One more fashion and perspective
 direction today.





[blog]:  https://github.com/kosbr/kosbr.github.io
[dockerfile]:  https://github.com/kosbr/kosbr.github.io/blob/master/Dockerfile
[dockercompose]:  https://docs.docker.com/compose/






